# DP-Decoding-in-LLM
Experiment a differentially private decoding strategy for Large Language Models


Differentially private (DP) decoding strategy is crucial in the context of language models (LLMs) due to privacy concerns. In many applications of LLMs, sensitive or private information may be present in the text data they are trained on or generate. Without privacy safeguards, there's a risk that the model could inadvertently reveal or leak this sensitive information, especially when generating responses or making predictions.


![BreadcrumbsDP-Decoding-in-LLM](https://github.com/OdedMous/DP-Decoding-in-LLM/blob/main/Utility-privacy%20tradeoff.png)



![BreadcrumbsDP-Decoding-in-LLM](https://github.com/OdedMous/DP-Decoding-in-LLM/blob/main/word%20probabilities.png)
